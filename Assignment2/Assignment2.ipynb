{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD0MARayrc4Y",
        "outputId": "8c657f3e-d9b1-4374-d279-d1b1eaacc54a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Libraries installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries, including the new langchain-community package\n",
        "!pip install -q langchain langchain-community huggingface_hub transformers accelerate bitsandbytes sentence-transformers faiss-cpu pypdf\n",
        "\n",
        "print(\"✅ Libraries installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvq2XxpetxMn",
        "outputId": "e1f9ea0d-4c70-44a2-fb83-9680a32feee1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.26.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "\n",
        "print(\"✅ Libraries imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-hcAnuFrkrj",
        "outputId": "5073efbf-85de-4ec3-c7c9-477ed5a7150c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Libraries imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p product_data\n",
        "\n",
        "# List the files to ensure they are uploaded correctly\n",
        "!ls product_data\n",
        "\n",
        "# Define the path to your data\n",
        "DATA_PATH = \"product_data/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe1ZV0FlrlUB",
        "outputId": "c36b209b-699b-4742-e6ff-fefb4d90facd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'2.1 Introduction to RAGs.pdf'\t'2.4 Indexation and Vector search.pdf'\n",
            "'2.2 Chunking.pdf'\t\t'2.5 Summary of RAGs.pdf'\n",
            "'2.3 Retrieval Methods.pdf'\t'2.6 Evaluation metrics for RAG systems.pdf'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load documents from the specified directory\n",
        "documents = []\n",
        "for filename in os.listdir(DATA_PATH):\n",
        "    if filename.endswith('.pdf'):\n",
        "        file_path = os.path.join(DATA_PATH, filename)\n",
        "        try:\n",
        "            loader = PyMuPDFLoader(file_path)\n",
        "            loaded_docs = loader.load()\n",
        "            for doc in loaded_docs:\n",
        "                doc.metadata['source'] = filename\n",
        "            documents.extend(loaded_docs)\n",
        "            print(f\"Loaded {len(loaded_docs)} pages from {filename}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {filename}: {e}\")\n",
        "\n",
        "print(f\"\\n✅ Total documents loaded: {len(documents)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QTBxhCytnYZ",
        "outputId": "3b70d8e4-78ce-4586-ea98-92450fa402c4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1 pages from 2.2 Chunking.pdf\n",
            "Loaded 2 pages from 2.6 Evaluation metrics for RAG systems.pdf\n",
            "Loaded 2 pages from 2.4 Indexation and Vector search.pdf\n",
            "Loaded 2 pages from 2.3 Retrieval Methods.pdf\n",
            "Loaded 2 pages from 2.5 Summary of RAGs.pdf\n",
            "Loaded 2 pages from 2.1 Introduction to RAGs.pdf\n",
            "\n",
            "✅ Total documents loaded: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Parameters ---\n",
        "CHUNK_SIZE = 1000\n",
        "CHUNK_OVERLAP = 150\n",
        "EMBEDDING_MODEL_NAME = \"BAAI/bge-large-en-v1.5\"\n",
        "LLM_MODEL_ID = \"google/flan-t5-large\"\n",
        "RETRIEVER_K = 3\n",
        "\n",
        "# Initialize the text splitter with our parameters\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP\n",
        ")\n",
        "\n",
        "# Split the documents into chunks\n",
        "chunked_docs = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Total documents chunked into {len(chunked_docs)} pieces.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWM4661Ett6p",
        "outputId": "1d8d4071-5c3a-4dcf-8026-f1180bcbafa6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total documents chunked into 17 pieces.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model and encoding kwargs\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "\n",
        "# Load embedding model and capture timing\n",
        "print(\"Loading embedding model...\")\n",
        "start_time = time.time()\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_NAME,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n",
        "embedding_load_time = time.time() - start_time\n",
        "print(f\"✅ Embedding model loaded in {embedding_load_time:.2f} seconds.\")\n",
        "\n",
        "# Create the FAISS vector store and capture timing\n",
        "print(\"\\nCreating vector store...\")\n",
        "start_time = time.time()\n",
        "db = FAISS.from_documents(chunked_docs, embeddings)\n",
        "db_creation_time = time.time() - start_time\n",
        "print(f\"✅ Vector store created in {db_creation_time:.2f} seconds.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMrqXDszt5A_",
        "outputId": "4ede4f5a-3f19-4c52-bc3a-c611e4bdf959"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embedding model...\n",
            "✅ Embedding model loaded in 8.82 seconds.\n",
            "\n",
            "Creating vector store...\n",
            "✅ Vector store created in 1.17 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the LLM and capture timing\n",
        "print(\"Loading LLM...\")\n",
        "start_time = time.time()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_ID)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(LLM_MODEL_ID, load_in_8bit=True, device_map=\"auto\")\n",
        "\n",
        "# Create a text-generation pipeline\n",
        "pipe = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=512,\n",
        "    temperature=0.1,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.15\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "llm_load_time = time.time() - start_time\n",
        "print(f\"✅ LLM loaded in {llm_load_time:.2f} seconds.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6xsDB9kuhmc",
        "outputId": "27662908-d942-4e49-b63e-accf7136fc8b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading LLM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Device set to use cuda:0\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ LLM loaded in 10.80 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom prompt template\n",
        "prompt_template = \"\"\"\n",
        "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Helpful Answer:\n",
        "\"\"\"\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Create the RetrievalQA chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(search_kwargs={\"k\": RETRIEVER_K}),\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": PROMPT}\n",
        ")\n",
        "\n",
        "print(\"✅ RAG chain created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FC1P0y-2unU2",
        "outputId": "5a3c6116-492f-47cf-9fc0-883a3d37e843"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ RAG chain created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define our list of questions\n",
        "queries = [\n",
        "    \"In the context of RAG systems, what does chunking mean?\",\n",
        "    \"Why is RAG needed instead of just retraining a model with new data?\",\n",
        "    \"What is re-ranking and how does it improve retrieval?\",\n",
        "    \"What is FAISS and what is it used for?\",\n",
        "    \"What is the difference between retrieval metrics and answer evaluation metrics?\"\n",
        "]\n",
        "\n",
        "results = []\n",
        "query_times = []\n",
        "\n",
        "for query in queries:\n",
        "    print(f\"--- Running Query: {query} ---\")\n",
        "    start_time = time.time()\n",
        "    result = qa_chain({\"query\": query})\n",
        "    query_time = time.time() - start_time\n",
        "\n",
        "    results.append(result)\n",
        "    query_times.append(query_time)\n",
        "\n",
        "    print(f\"Answer: {result['result']}\")\n",
        "    print(f\"Time Taken: {query_time:.2f} seconds\\n\")\n",
        "\n",
        "print(\"✅ All queries processed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UshEyDtyu__Y",
        "outputId": "a0bf2796-f33d-47d9-a9bd-0dab0f483d01"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (672 > 512). Running this sequence through the model will result in indexing errors\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running Query: In the context of RAG systems, what does chunking mean? ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: splitting the text into different segments and we store these different segments\n",
            "Time Taken: 6.35 seconds\n",
            "\n",
            "--- Running Query: Why is RAG needed instead of just retraining a model with new data? ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Catastrophic forgetting of neural networks\n",
            "Time Taken: 9.89 seconds\n",
            "\n",
            "--- Running Query: What is re-ranking and how does it improve retrieval? ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Re-ranking is a post retrieval technique which can be used to improve the relevancy of the top k chunks being passed to the generation model\n",
            "Time Taken: 33.53 seconds\n",
            "\n",
            "--- Running Query: What is FAISS and what is it used for? ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: I don’t know\n",
            "Time Taken: 2.92 seconds\n",
            "\n",
            "--- Running Query: What is the difference between retrieval metrics and answer evaluation metrics? ---\n",
            "Answer: The answer generated by the RAG system should also be evaluated because sometimes we may not choose the correct answer from the retrieved context.\n",
            "Time Taken: 7.46 seconds\n",
            "\n",
            "✅ All queries processed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Automated Report Generation ---\n",
        "\n",
        "# Calculate average query time\n",
        "avg_query_time = sum(query_times) / len(query_times) if query_times else 0\n",
        "\n",
        "# Start building the report string\n",
        "report = f\"\"\"\n",
        "RAG System Design and Experiment Report\n",
        "=======================================\n",
        "\n",
        "Section 1: Metadata about Document Chunks\n",
        "-----------------------------------------\n",
        "1.  **Chunking Method Used**: RecursiveCharacterTextSplitter from LangChain.\n",
        "2.  **Splitting Technique Details**: This splitter uses a hierarchy of separators (e.g., \"\\\\n\\\\n\", \"\\\\n\", \" \") to keep semantically related text together.\n",
        "3.  **Chunk Size Measurement Criteria**: Number of characters.\n",
        "4.  **Chunk Size Used**: {CHUNK_SIZE} characters.\n",
        "5.  **Chunk Overlapping**: {CHUNK_OVERLAP} characters.\n",
        "6.  **Chunk Preprocessing**: No special preprocessing was applied beyond the text extraction handled by PyMuPDFLoader.\n",
        "\n",
        "---\n",
        "\n",
        "Section 2: DB and Model Details\n",
        "-------------------------------\n",
        "1.  **Database Used**: FAISS (Facebook AI Similarity Search) in-memory vector store.\n",
        "2.  **Embedding Model**: {EMBEDDING_MODEL_NAME}\n",
        "3.  **LLM Used**: {LLM_MODEL_ID}\n",
        "4.  **Processing Latencies**:\n",
        "    * **Embedding Model Load Time**: {embedding_load_time:.2f} seconds.\n",
        "    * **LLM Load Time**: {llm_load_time:.2f} seconds.\n",
        "    * **Database Creation Time (Vectorization & Indexing)**: {db_creation_time:.2f} seconds.\n",
        "    * **Average Query Time (End-to-End)**: {avg_query_time:.2f} seconds.\n",
        "5.  **Hardware Usage**: Models were configured to run on GPU if available ('{model_kwargs['device']}').\n",
        "\n",
        "---\n",
        "\n",
        "Section 3: LLM / Model Evaluation Parameters\n",
        "---------------------------------------------\n",
        "1.  **LLM Framework**: HuggingFacePipeline in LangChain.\n",
        "2.  **Tokens per Chunk (Approximate)**: {CHUNK_SIZE // 4} tokens. With k={RETRIEVER_K}, context is ~{RETRIEVER_K * (CHUNK_SIZE // 4)} tokens.\n",
        "3.  **Decoding Strategy**:\n",
        "    * **max_length**: 512\n",
        "    * **temperature**: 0.1\n",
        "    * **top_p**: 0.95\n",
        "    * **repetition_penalty**: 1.15\n",
        "\n",
        "---\n",
        "\n",
        "Section 4: Experiment Results on the Best Parameters\n",
        "--------------------------------------------------\n",
        "1.  **Prompt Used**:\n",
        "    ```\n",
        "    {prompt_template}\n",
        "    ```\n",
        "2.  **Chain Type Used**: `stuff`\n",
        "3.  **Best Chunk Parameters**: `chunk_size`: {CHUNK_SIZE}, `chunk_overlap`: {CHUNK_OVERLAP}.\n",
        "4.  **Retriever Parameters**: FAISS retriever with `k={RETRIEVER_K}`.\n",
        "\n",
        "5.  **Five Example Results**:\n",
        "\"\"\"\n",
        "\n",
        "# Append the query results to the report\n",
        "for i, result in enumerate(results):\n",
        "    report += f\"\"\"\n",
        "    -----------------------------------\n",
        "    **Example {i+1}**:\n",
        "    * **Query**: {result['query']}\n",
        "    * **Answer**: {result['result']}\n",
        "    * **Source Docs**: {[doc.metadata.get('source', 'N/A') for doc in result['source_documents']]}\n",
        "    * **Time Taken**: {query_times[i]:.2f} seconds\n",
        "\"\"\"\n",
        "\n",
        "# Append the final section on metrics\n",
        "report += \"\"\"\n",
        "6.  **Retrieval and Answer Level Metrics (Manual Evaluation)**:\n",
        "    * **Instructions**: For each example above, manually judge the results.\n",
        "    * **Context Precision**: (Relevant Retrieved Docs) / (Total Retrieved Docs). Did the sources contain the right info?\n",
        "    * **Context Recall**: (Relevant Retrieved Docs) / (Total Relevant Docs). Did we find all the possible sources?\n",
        "    * **Answer Faithfulness**: Does the answer stick strictly to the provided context? (Score 1 for Yes, 0 for No).\n",
        "    * **Answer Relevancy**: Is the answer relevant to the question? (Score 1 for Yes, 0 for No).\n",
        "\"\"\"\n",
        "\n",
        "# Print the final report\n",
        "print(report)\n",
        "\n",
        "# Optionally, save the report to a file\n",
        "with open(\"rag_report.txt\", \"w\") as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(\"\\n\\n✅ Report generated and saved to rag_report.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "501ykFMIxf4k",
        "outputId": "64d3d001-fd01-4481-835f-8893ba8b19de"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RAG System Design and Experiment Report\n",
            "=======================================\n",
            "\n",
            "Section 1: Metadata about Document Chunks\n",
            "-----------------------------------------\n",
            "1.  **Chunking Method Used**: RecursiveCharacterTextSplitter from LangChain.\n",
            "2.  **Splitting Technique Details**: This splitter uses a hierarchy of separators (e.g., \"\\n\\n\", \"\\n\", \" \") to keep semantically related text together.\n",
            "3.  **Chunk Size Measurement Criteria**: Number of characters.\n",
            "4.  **Chunk Size Used**: 1000 characters.\n",
            "5.  **Chunk Overlapping**: 150 characters.\n",
            "6.  **Chunk Preprocessing**: No special preprocessing was applied beyond the text extraction handled by PyMuPDFLoader.\n",
            "\n",
            "---\n",
            "\n",
            "Section 2: DB and Model Details\n",
            "-------------------------------\n",
            "1.  **Database Used**: FAISS (Facebook AI Similarity Search) in-memory vector store.\n",
            "2.  **Embedding Model**: BAAI/bge-large-en-v1.5\n",
            "3.  **LLM Used**: google/flan-t5-large\n",
            "4.  **Processing Latencies**:\n",
            "    * **Embedding Model Load Time**: 8.82 seconds.\n",
            "    * **LLM Load Time**: 10.80 seconds.\n",
            "    * **Database Creation Time (Vectorization & Indexing)**: 1.17 seconds.\n",
            "    * **Average Query Time (End-to-End)**: 12.03 seconds.\n",
            "5.  **Hardware Usage**: Models were configured to run on GPU if available ('cuda').\n",
            "\n",
            "---\n",
            "\n",
            "Section 3: LLM / Model Evaluation Parameters\n",
            "---------------------------------------------\n",
            "1.  **LLM Framework**: HuggingFacePipeline in LangChain.\n",
            "2.  **Tokens per Chunk (Approximate)**: 250 tokens. With k=3, context is ~750 tokens.\n",
            "3.  **Decoding Strategy**:\n",
            "    * **max_length**: 512\n",
            "    * **temperature**: 0.1\n",
            "    * **top_p**: 0.95\n",
            "    * **repetition_penalty**: 1.15\n",
            "\n",
            "---\n",
            "\n",
            "Section 4: Experiment Results on the Best Parameters\n",
            "--------------------------------------------------\n",
            "1.  **Prompt Used**:\n",
            "    ```\n",
            "    \n",
            "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "Context: {context}\n",
            "\n",
            "Question: {question}\n",
            "\n",
            "Helpful Answer:\n",
            "\n",
            "    ```\n",
            "2.  **Chain Type Used**: `stuff`\n",
            "3.  **Best Chunk Parameters**: `chunk_size`: 1000, `chunk_overlap`: 150.\n",
            "4.  **Retriever Parameters**: FAISS retriever with `k=3`.\n",
            "\n",
            "5.  **Five Example Results**:\n",
            "\n",
            "    -----------------------------------\n",
            "    **Example 1**:\n",
            "    * **Query**: In the context of RAG systems, what does chunking mean?\n",
            "    * **Answer**: splitting the text into different segments and we store these different segments\n",
            "    * **Source Docs**: ['2.2 Chunking.pdf', '2.5 Summary of RAGs.pdf', '2.4 Indexation and Vector search.pdf']\n",
            "    * **Time Taken**: 6.35 seconds\n",
            "\n",
            "    -----------------------------------\n",
            "    **Example 2**:\n",
            "    * **Query**: Why is RAG needed instead of just retraining a model with new data?\n",
            "    * **Answer**: Catastrophic forgetting of neural networks\n",
            "    * **Source Docs**: ['2.1 Introduction to RAGs.pdf', '2.1 Introduction to RAGs.pdf', '2.3 Retrieval Methods.pdf']\n",
            "    * **Time Taken**: 9.89 seconds\n",
            "\n",
            "    -----------------------------------\n",
            "    **Example 3**:\n",
            "    * **Query**: What is re-ranking and how does it improve retrieval?\n",
            "    * **Answer**: Re-ranking is a post retrieval technique which can be used to improve the relevancy of the top k chunks being passed to the generation model\n",
            "    * **Source Docs**: ['2.3 Retrieval Methods.pdf', '2.3 Retrieval Methods.pdf', '2.3 Retrieval Methods.pdf']\n",
            "    * **Time Taken**: 33.53 seconds\n",
            "\n",
            "    -----------------------------------\n",
            "    **Example 4**:\n",
            "    * **Query**: What is FAISS and what is it used for?\n",
            "    * **Answer**: I don’t know\n",
            "    * **Source Docs**: ['2.5 Summary of RAGs.pdf', '2.5 Summary of RAGs.pdf', '2.1 Introduction to RAGs.pdf']\n",
            "    * **Time Taken**: 2.92 seconds\n",
            "\n",
            "    -----------------------------------\n",
            "    **Example 5**:\n",
            "    * **Query**: What is the difference between retrieval metrics and answer evaluation metrics?\n",
            "    * **Answer**: The answer generated by the RAG system should also be evaluated because sometimes we may not choose the correct answer from the retrieved context.\n",
            "    * **Source Docs**: ['2.6 Evaluation metrics for RAG systems.pdf', '2.3 Retrieval Methods.pdf', '2.3 Retrieval Methods.pdf']\n",
            "    * **Time Taken**: 7.46 seconds\n",
            "\n",
            "6.  **Retrieval and Answer Level Metrics (Manual Evaluation)**:\n",
            "    * **Instructions**: For each example above, manually judge the results.\n",
            "    * **Context Precision**: (Relevant Retrieved Docs) / (Total Retrieved Docs). Did the sources contain the right info?\n",
            "    * **Context Recall**: (Relevant Retrieved Docs) / (Total Relevant Docs). Did we find all the possible sources?\n",
            "    * **Answer Faithfulness**: Does the answer stick strictly to the provided context? (Score 1 for Yes, 0 for No).\n",
            "    * **Answer Relevancy**: Is the answer relevant to the question? (Score 1 for Yes, 0 for No).\n",
            "\n",
            "\n",
            "\n",
            "✅ Report generated and saved to rag_report.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pos0u862xvGd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
