
RAG System Design and Experiment Report
=======================================

Section 1: Metadata about Document Chunks
-----------------------------------------
1.  **Chunking Method Used**: RecursiveCharacterTextSplitter from LangChain.
2.  **Splitting Technique Details**: This splitter uses a hierarchy of separators (e.g., "\n\n", "\n", " ") to keep semantically related text together.
3.  **Chunk Size Measurement Criteria**: Number of characters.
4.  **Chunk Size Used**: 1000 characters.
5.  **Chunk Overlapping**: 150 characters.
6.  **Chunk Preprocessing**: No special preprocessing was applied beyond the text extraction handled by PyMuPDFLoader.

---

Section 2: DB and Model Details
-------------------------------
1.  **Database Used**: FAISS (Facebook AI Similarity Search) in-memory vector store.
2.  **Embedding Model**: BAAI/bge-large-en-v1.5
3.  **LLM Used**: google/flan-t5-large
4.  **Processing Latencies**:
    * **Embedding Model Load Time**: 8.82 seconds.
    * **LLM Load Time**: 10.80 seconds.
    * **Database Creation Time (Vectorization & Indexing)**: 1.17 seconds.
    * **Average Query Time (End-to-End)**: 12.03 seconds.
5.  **Hardware Usage**: Models were configured to run on GPU if available ('cuda').

---

Section 3: LLM / Model Evaluation Parameters
---------------------------------------------
1.  **LLM Framework**: HuggingFacePipeline in LangChain.
2.  **Tokens per Chunk (Approximate)**: 250 tokens. With k=3, context is ~750 tokens.
3.  **Decoding Strategy**:
    * **max_length**: 512
    * **temperature**: 0.1
    * **top_p**: 0.95
    * **repetition_penalty**: 1.15

---

Section 4: Experiment Results on the Best Parameters
--------------------------------------------------
1.  **Prompt Used**:
    ```
    
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

Context: {context}

Question: {question}

Helpful Answer:

    ```
2.  **Chain Type Used**: `stuff`
3.  **Best Chunk Parameters**: `chunk_size`: 1000, `chunk_overlap`: 150.
4.  **Retriever Parameters**: FAISS retriever with `k=3`.

5.  **Five Example Results**:

    -----------------------------------
    **Example 1**:
    * **Query**: In the context of RAG systems, what does chunking mean?
    * **Answer**: splitting the text into different segments and we store these different segments
    * **Source Docs**: ['2.2 Chunking.pdf', '2.5 Summary of RAGs.pdf', '2.4 Indexation and Vector search.pdf']
    * **Time Taken**: 6.35 seconds

    -----------------------------------
    **Example 2**:
    * **Query**: Why is RAG needed instead of just retraining a model with new data?
    * **Answer**: Catastrophic forgetting of neural networks
    * **Source Docs**: ['2.1 Introduction to RAGs.pdf', '2.1 Introduction to RAGs.pdf', '2.3 Retrieval Methods.pdf']
    * **Time Taken**: 9.89 seconds

    -----------------------------------
    **Example 3**:
    * **Query**: What is re-ranking and how does it improve retrieval?
    * **Answer**: Re-ranking is a post retrieval technique which can be used to improve the relevancy of the top k chunks being passed to the generation model
    * **Source Docs**: ['2.3 Retrieval Methods.pdf', '2.3 Retrieval Methods.pdf', '2.3 Retrieval Methods.pdf']
    * **Time Taken**: 33.53 seconds

    -----------------------------------
    **Example 4**:
    * **Query**: What is FAISS and what is it used for?
    * **Answer**: I donâ€™t know
    * **Source Docs**: ['2.5 Summary of RAGs.pdf', '2.5 Summary of RAGs.pdf', '2.1 Introduction to RAGs.pdf']
    * **Time Taken**: 2.92 seconds

    -----------------------------------
    **Example 5**:
    * **Query**: What is the difference between retrieval metrics and answer evaluation metrics?
    * **Answer**: The answer generated by the RAG system should also be evaluated because sometimes we may not choose the correct answer from the retrieved context.
    * **Source Docs**: ['2.6 Evaluation metrics for RAG systems.pdf', '2.3 Retrieval Methods.pdf', '2.3 Retrieval Methods.pdf']
    * **Time Taken**: 7.46 seconds

6.  **Retrieval and Answer Level Metrics (Manual Evaluation)**:
    * **Instructions**: For each example above, manually judge the results.
    * **Context Precision**: (Relevant Retrieved Docs) / (Total Retrieved Docs). Did the sources contain the right info?
    * **Context Recall**: (Relevant Retrieved Docs) / (Total Relevant Docs). Did we find all the possible sources?
    * **Answer Faithfulness**: Does the answer stick strictly to the provided context? (Score 1 for Yes, 0 for No).
    * **Answer Relevancy**: Is the answer relevant to the question? (Score 1 for Yes, 0 for No).
