{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c5ac231acf84d6aaed317f776b54963": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f7d0d5e1513947a4ade4035833024a18",
              "IPY_MODEL_486c925cfabd4664bf04715703b3da77",
              "IPY_MODEL_185b6048cdab42348d509cf13df798fa"
            ],
            "layout": "IPY_MODEL_f075801828ac4d959072f40f3530af6d"
          }
        },
        "f7d0d5e1513947a4ade4035833024a18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70914dac97f74baaab9fdebf10e932d6",
            "placeholder": "​",
            "style": "IPY_MODEL_7f1b8f5aaec748bb8b8a6e9f4b156837",
            "value": "Generating Insights: 100%"
          }
        },
        "486c925cfabd4664bf04715703b3da77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af2e9f44fbaf413187892f341d123fd7",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_47fb25913e6a42db9c6e287189c5001b",
            "value": 50
          }
        },
        "185b6048cdab42348d509cf13df798fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a3454a5bc894442a76baa860aa92b2f",
            "placeholder": "​",
            "style": "IPY_MODEL_2df462bfae914d47bd3fe3a971faa518",
            "value": " 50/50 [02:16&lt;00:00,  1.87s/it]"
          }
        },
        "f075801828ac4d959072f40f3530af6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70914dac97f74baaab9fdebf10e932d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f1b8f5aaec748bb8b8a6e9f4b156837": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af2e9f44fbaf413187892f341d123fd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47fb25913e6a42db9c6e287189c5001b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3a3454a5bc894442a76baa860aa92b2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2df462bfae914d47bd3fe3a971faa518": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "by6dioxrwHIv",
        "outputId": "b3d2d041-f60a-473c-e8d5-25954020358d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ STEP 1: Generating mock data files...\n",
            "... data_dictionary.csv has been created.\n",
            "... train.csv has been created.\n",
            "... test.csv has been created.\n",
            "\n",
            "✅ Mock data generation is complete. The notebook is ready to run.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"✅ STEP 1: Generating mock data files...\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create the data dictionary to explain the columns\n",
        "data_dict_data = {\n",
        "    'Column Name': ['machine_id', 'cpu_usage_percent', 'memory_usage_gb', 'disk_io_mbps', 'error_count', 'insight'],\n",
        "    'Description': [\n",
        "        'Unique identifier for the machine.',\n",
        "        'CPU utilization as a percentage.',\n",
        "        'Memory currently in use (Gigabytes).',\n",
        "        'Disk read/write speed (Megabytes per second).',\n",
        "        'Number of critical errors logged in the period.',\n",
        "        'A human-readable, actionable insight based on the log data.'\n",
        "    ]\n",
        "}\n",
        "data_dict_df = pd.DataFrame(data_dict_data)\n",
        "data_dict_df.to_csv('data_dictionary.csv', index=False)\n",
        "print(\"... data_dictionary.csv has been created.\")\n",
        "\n",
        "# Define a function to generate realistic log data and corresponding insights\n",
        "def generate_data(num_rows):\n",
        "    \"\"\"Creates a DataFrame with synthetic machine logs and insights.\"\"\"\n",
        "    data = []\n",
        "    for i in range(num_rows):\n",
        "        machine_id = f'PROD-SRV-{1001 + i}'\n",
        "        # Introduce anomalous or normal values for different metrics\n",
        "        cpu = round(np.random.choice([np.random.uniform(30, 60), np.random.uniform(92, 99)]), 2)\n",
        "        mem = round(np.random.choice([np.random.uniform(10, 20), np.random.uniform(28, 31.5)]), 2)\n",
        "        disk = round(np.random.uniform(20, 250), 2)\n",
        "        errors = np.random.choice([0, 0, 0, 0, 0, 1, 3, 10])\n",
        "\n",
        "        insight = \"\"\n",
        "        # Create insights that logically follow the data\n",
        "        if errors > 5:\n",
        "            insight = f\"Critical alert: {errors} errors detected on {machine_id}. Immediate investigation is required to ensure system stability.\"\n",
        "        elif cpu > 90:\n",
        "            insight = f\"High CPU usage ({cpu}%) on {machine_id} suggests a potential performance bottleneck. Recommend analyzing top processes.\"\n",
        "        elif mem > 27:\n",
        "            insight = f\"Memory usage is critical at {mem}GB on {machine_id}. Check for memory leaks or consider a resource upgrade.\"\n",
        "        elif disk > 220:\n",
        "             insight = f\"Unusually high disk I/O ({disk} MB/s) on {machine_id} could degrade storage performance. Monitor read/write operations.\"\n",
        "        else:\n",
        "            insight = f\"System {machine_id} is operating within normal performance parameters. No action is required at this time.\"\n",
        "        data.append([machine_id, cpu, mem, disk, errors, insight])\n",
        "\n",
        "    columns = ['machine_id', 'cpu_usage_percent', 'memory_usage_gb', 'disk_io_mbps', 'error_count', 'insight']\n",
        "    return pd.DataFrame(data, columns=columns)\n",
        "\n",
        "# Generate and save the training and testing datasets\n",
        "train_df = generate_data(200)\n",
        "train_df.to_csv('train.csv', index=False)\n",
        "print(\"... train.csv has been created.\")\n",
        "\n",
        "test_df = generate_data(50)\n",
        "test_df.to_csv('test.csv', index=False)\n",
        "print(\"... test.csv has been created.\")\n",
        "\n",
        "print(\"\\n✅ Mock data generation is complete. The notebook is ready to run.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"✅ STEP 2: Installing Python libraries...\")\n",
        "!pip install transformers sentence-transformers evaluate rouge_score nltk --quiet\n",
        "print(\"... Installation complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnjHvvFHwSQP",
        "outputId": "1fdede71-cc03-4203-fb65-3dd338a92af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ STEP 2: Installing Python libraries...\n",
            "... Installation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"✅ STEP 3: Importing libraries and loading data...\")\n",
        "\n",
        "# Core data handling and progress bars\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Hugging Face and evaluation libraries\n",
        "from transformers import pipeline\n",
        "import evaluate\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# --- Data Loading ---\n",
        "try:\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    data_dict_df = pd.read_csv('data_dictionary.csv')\n",
        "\n",
        "    print(\"... Datasets loaded successfully.\")\n",
        "\n",
        "    # --- Data Exploration ---\n",
        "    print(\"\\n--- Data Dictionary ---\")\n",
        "    display(data_dict_df)\n",
        "\n",
        "    print(\"\\n--- Test Data Sample ---\")\n",
        "    display(test_df.head())\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"❌ ERROR: Could not find the data files\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "id": "Mbqy1zShwWgD",
        "outputId": "b3269a35-cd81-4703-942b-aac7f694eee8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ STEP 3: Importing libraries and loading data...\n",
            "... Datasets loaded successfully.\n",
            "\n",
            "--- Data Dictionary ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "         Column Name                                                  Description\n",
              "0         machine_id                           Unique identifier for the machine.\n",
              "1  cpu_usage_percent                             CPU utilization as a percentage.\n",
              "2    memory_usage_gb                         Memory currently in use (Gigabytes).\n",
              "3       disk_io_mbps                Disk read/write speed (Megabytes per second).\n",
              "4        error_count              Number of critical errors logged in the period.\n",
              "5            insight  A human-readable, actionable insight based on the log data."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-086f9d5c-51fa-4b7b-bac0-d95336495aa3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Column Name</th>\n",
              "      <th>Description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>machine_id</td>\n",
              "      <td>Unique identifier for the machine.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cpu_usage_percent</td>\n",
              "      <td>CPU utilization as a percentage.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>memory_usage_gb</td>\n",
              "      <td>Memory currently in use (Gigabytes).</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>disk_io_mbps</td>\n",
              "      <td>Disk read/write speed (Megabytes per second).</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>error_count</td>\n",
              "      <td>Number of critical errors logged in the period.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>insight</td>\n",
              "      <td>A human-readable, actionable insight based on the log data.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-086f9d5c-51fa-4b7b-bac0-d95336495aa3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-086f9d5c-51fa-4b7b-bac0-d95336495aa3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-086f9d5c-51fa-4b7b-bac0-d95336495aa3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-54e486c5-299e-474c-b9e7-06fe68bc98c8\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-54e486c5-299e-474c-b9e7-06fe68bc98c8')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-54e486c5-299e-474c-b9e7-06fe68bc98c8 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_f36cb2fc-6c58-47e6-9925-f5a9a3df1abb\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data_dict_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f36cb2fc-6c58-47e6-9925-f5a9a3df1abb button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data_dict_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data_dict_df",
              "summary": "{\n  \"name\": \"data_dict_df\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"Column Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"machine_id\",\n          \"cpu_usage_percent\",\n          \"insight\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Unique identifier for the machine.\",\n          \"CPU utilization as a percentage.\",\n          \"A human-readable, actionable insight based on the log data.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Test Data Sample ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      machine_id  cpu_usage_percent  memory_usage_gb  disk_io_mbps  error_count  \\\n",
              "0  PROD-SRV-1001              95.10            16.94        124.28            0   \n",
              "1  PROD-SRV-1002              32.25            13.95        237.53            0   \n",
              "2  PROD-SRV-1003              97.75            30.24        236.14           10   \n",
              "3  PROD-SRV-1004              92.05            19.46         62.51            1   \n",
              "4  PROD-SRV-1005              51.78            17.61         91.28            0   \n",
              "\n",
              "                                                                                                                    insight  \n",
              "0   High CPU usage (95.1%) on PROD-SRV-1001 suggests a potential performance bottleneck. Recommend analyzing top processes.  \n",
              "1  Unusually high disk I/O (237.53 MB/s) on PROD-SRV-1002 could degrade storage performance. Monitor read/write operations.  \n",
              "2      Critical alert: 10 errors detected on PROD-SRV-1003. Immediate investigation is required to ensure system stability.  \n",
              "3  High CPU usage (92.05%) on PROD-SRV-1004 suggests a potential performance bottleneck. Recommend analyzing top processes.  \n",
              "4               System PROD-SRV-1005 is operating within normal performance parameters. No action is required at this time.  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-14c0e5ca-1462-4853-a278-f1b6e732fb43\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>machine_id</th>\n",
              "      <th>cpu_usage_percent</th>\n",
              "      <th>memory_usage_gb</th>\n",
              "      <th>disk_io_mbps</th>\n",
              "      <th>error_count</th>\n",
              "      <th>insight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>PROD-SRV-1001</td>\n",
              "      <td>95.10</td>\n",
              "      <td>16.94</td>\n",
              "      <td>124.28</td>\n",
              "      <td>0</td>\n",
              "      <td>High CPU usage (95.1%) on PROD-SRV-1001 suggests a potential performance bottleneck. Recommend analyzing top processes.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>PROD-SRV-1002</td>\n",
              "      <td>32.25</td>\n",
              "      <td>13.95</td>\n",
              "      <td>237.53</td>\n",
              "      <td>0</td>\n",
              "      <td>Unusually high disk I/O (237.53 MB/s) on PROD-SRV-1002 could degrade storage performance. Monitor read/write operations.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>PROD-SRV-1003</td>\n",
              "      <td>97.75</td>\n",
              "      <td>30.24</td>\n",
              "      <td>236.14</td>\n",
              "      <td>10</td>\n",
              "      <td>Critical alert: 10 errors detected on PROD-SRV-1003. Immediate investigation is required to ensure system stability.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PROD-SRV-1004</td>\n",
              "      <td>92.05</td>\n",
              "      <td>19.46</td>\n",
              "      <td>62.51</td>\n",
              "      <td>1</td>\n",
              "      <td>High CPU usage (92.05%) on PROD-SRV-1004 suggests a potential performance bottleneck. Recommend analyzing top processes.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PROD-SRV-1005</td>\n",
              "      <td>51.78</td>\n",
              "      <td>17.61</td>\n",
              "      <td>91.28</td>\n",
              "      <td>0</td>\n",
              "      <td>System PROD-SRV-1005 is operating within normal performance parameters. No action is required at this time.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-14c0e5ca-1462-4853-a278-f1b6e732fb43')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-14c0e5ca-1462-4853-a278-f1b6e732fb43 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-14c0e5ca-1462-4853-a278-f1b6e732fb43');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-578a1ffe-450a-4659-a045-7e01e84e3084\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-578a1ffe-450a-4659-a045-7e01e84e3084')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-578a1ffe-450a-4659-a045-7e01e84e3084 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(f\\\"\\u274c ERROR: Could not find the data files\\\")\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"machine_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"PROD-SRV-1002\",\n          \"PROD-SRV-1005\",\n          \"PROD-SRV-1003\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cpu_usage_percent\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29.88158680525517,\n        \"min\": 32.25,\n        \"max\": 97.75,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          32.25,\n          51.78,\n          97.75\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"memory_usage_gb\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.2485878404644355,\n        \"min\": 13.95,\n        \"max\": 30.24,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          13.95,\n          17.61,\n          30.24\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"disk_io_mbps\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 81.9222983686371,\n        \"min\": 62.51,\n        \"max\": 237.53,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          237.53,\n          91.28,\n          236.14\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"error_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 0,\n        \"max\": 10,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0,\n          10,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"insight\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Unusually high disk I/O (237.53 MB/s) on PROD-SRV-1002 could degrade storage performance. Monitor read/write operations.\",\n          \"System PROD-SRV-1005 is operating within normal performance parameters. No action is required at this time.\",\n          \"Critical alert: 10 errors detected on PROD-SRV-1003. Immediate investigation is required to ensure system stability.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"✅ STEP 4: Loading the pre-trained model...\")\n",
        "\n",
        "# Define the model checkpoint from Hugging Face Hub\n",
        "MODEL_NAME = \"google/flan-t5-large\"\n",
        "\n",
        "# Set device to GPU (cuda) if available, otherwise CPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Create the text-to-text generation pipeline\n",
        "summarizer_pipeline = pipeline(\n",
        "    task=\"text2text-generation\",\n",
        "    model=MODEL_NAME,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(f\"... Model '{MODEL_NAME}' loaded successfully on device: '{device}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WI_SEtqpw0YF",
        "outputId": "400c2c12-8f58-4883-f904-1eeb41727464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ STEP 4: Loading the pre-trained model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "... Model 'google/flan-t5-large' loaded successfully on device: 'cuda'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"✅ STEP 5: Defining the prompt engineering function...\")\n",
        "\n",
        "def create_prompt(data_row: pd.Series) -> str:\n",
        "    \"\"\"\n",
        "    Converts a pandas Series into a structured, few-shot prompt.\n",
        "    \"\"\"\n",
        "    # --- FEW-SHOT EXAMPLES ---\n",
        "    # Provide clear examples of the desired input-output format.\n",
        "    examples = \"\"\"\n",
        "Analyze the following machine usage log and generate a concise, actionable business insight.\n",
        "\n",
        "---\n",
        "**Example Input:**\n",
        "Machine Id: PROD-SRV-9001\n",
        "Cpu Usage Percent: 95.8\n",
        "Error Count: 0\n",
        "\n",
        "**Example Insight:**\n",
        "High CPU usage (95.8%) on PROD-SRV-9001 suggests a potential performance bottleneck. Recommend analyzing top processes.\n",
        "---\n",
        "**Example Input:**\n",
        "Machine Id: PROD-SRV-9002\n",
        "Cpu Usage Percent: 45.1\n",
        "Error Count: 8\n",
        "\n",
        "**Example Insight:**\n",
        "Critical alert: 8 errors detected on PROD-SRV-9002. Immediate investigation is required to ensure system stability.\n",
        "---\n",
        "\"\"\"\n",
        "\n",
        "    # --- ACTUAL TASK ---\n",
        "    # Format the current row's data for the actual task.\n",
        "    current_log = \"\"\n",
        "    for col, value in data_row.drop('insight').items():\n",
        "        current_log += f\"{col.replace('_', ' ').title()}: {value}\\n\"\n",
        "\n",
        "    # Combine examples with the actual task.\n",
        "    final_prompt = examples + f\"\"\"**Actual Input:**\n",
        "{current_log}\n",
        "**Actual Insight:**\"\"\"\n",
        "\n",
        "    return final_prompt\n",
        "\n",
        "# Display an example prompt from the first row of the test data\n",
        "example_prompt = create_prompt(test_df.iloc[0])\n",
        "print(\"... Prompt function created. Example prompt:\\n\")\n",
        "print(example_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFyaA2n1xBEW",
        "outputId": "9be8b1a1-6ea5-48bd-dfbe-e5a6ca471c9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ STEP 5: Defining the prompt engineering function...\n",
            "... Prompt function created. Example prompt:\n",
            "\n",
            "\n",
            "Analyze the following machine usage log and generate a concise, actionable business insight.\n",
            "\n",
            "---\n",
            "**Example Input:**\n",
            "Machine Id: PROD-SRV-9001\n",
            "Cpu Usage Percent: 95.8\n",
            "Error Count: 0\n",
            "\n",
            "**Example Insight:**\n",
            "High CPU usage (95.8%) on PROD-SRV-9001 suggests a potential performance bottleneck. Recommend analyzing top processes.\n",
            "---\n",
            "**Example Input:**\n",
            "Machine Id: PROD-SRV-9002\n",
            "Cpu Usage Percent: 45.1\n",
            "Error Count: 8\n",
            "\n",
            "**Example Insight:**\n",
            "Critical alert: 8 errors detected on PROD-SRV-9002. Immediate investigation is required to ensure system stability.\n",
            "---\n",
            "**Actual Input:**\n",
            "Machine Id: PROD-SRV-1001\n",
            "Cpu Usage Percent: 95.1\n",
            "Memory Usage Gb: 16.94\n",
            "Disk Io Mbps: 124.28\n",
            "Error Count: 0\n",
            "\n",
            "**Actual Insight:**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"✅ STEP 6: Running inference on the test data...\")\n",
        "\n",
        "# We will run this on the full test set (50 samples)\n",
        "# This may take a few minutes depending on the Colab GPU allocated.\n",
        "test_subset_df = test_df\n",
        "\n",
        "# Lists to store the results\n",
        "generated_insights = []\n",
        "reference_insights = test_subset_df['insight'].tolist()\n",
        "\n",
        "# Use tqdm for a live progress bar\n",
        "for index, row in tqdm(test_subset_df.iterrows(), total=test_subset_df.shape[0], desc=\"Generating Insights\"):\n",
        "    prompt = create_prompt(row)\n",
        "\n",
        "    # Get model output\n",
        "    output = summarizer_pipeline(\n",
        "    prompt,\n",
        "    max_length=128,\n",
        "    num_beams=5,\n",
        "    temperature=0.9,       # Increase from default 1.0, makes output less predictable\n",
        "    length_penalty=2.0,    # Encourages the model to generate longer, more complete sentences\n",
        "    no_repeat_ngram_size=2,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "    generated_insights.append(output[0]['generated_text'])\n",
        "\n",
        "print(f\"\\n... Inference complete. Generated {len(generated_insights)} insights.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0c5ac231acf84d6aaed317f776b54963",
            "f7d0d5e1513947a4ade4035833024a18",
            "486c925cfabd4664bf04715703b3da77",
            "185b6048cdab42348d509cf13df798fa",
            "f075801828ac4d959072f40f3530af6d",
            "70914dac97f74baaab9fdebf10e932d6",
            "7f1b8f5aaec748bb8b8a6e9f4b156837",
            "af2e9f44fbaf413187892f341d123fd7",
            "47fb25913e6a42db9c6e287189c5001b",
            "3a3454a5bc894442a76baa860aa92b2f",
            "2df462bfae914d47bd3fe3a971faa518"
          ]
        },
        "id": "XQyAvpvUxcpK",
        "outputId": "f90e5616-53b5-434f-e486-21f2f736052b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ STEP 6: Running inference on the test data...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating Insights:   0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c5ac231acf84d6aaed317f776b54963"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=128) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "... Inference complete. Generated 50 insights.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"✅ STEP 7: Calculating lexical similarity metrics...\")\n",
        "\n",
        "# Load the evaluation metrics from the 'evaluate' library\n",
        "rouge_metric = evaluate.load('rouge')\n",
        "bleu_metric = evaluate.load('bleu')\n",
        "\n",
        "# --- ROUGE Calculation ---\n",
        "rouge_results = rouge_metric.compute(predictions=generated_insights, references=reference_insights)\n",
        "print(\"\\n--- ROUGE Scores ---\")\n",
        "for key, value in rouge_results.items():\n",
        "    print(f\"  {key:<8}: {value:.4f}\")\n",
        "\n",
        "# --- BLEU Calculation ---\n",
        "# BLEU requires references to be in a list of lists\n",
        "bleu_results = bleu_metric.compute(predictions=generated_insights, references=[[ref] for ref in reference_insights])\n",
        "print(\"\\n--- BLEU Score ---\")\n",
        "print(f\"  BLEU    : {bleu_results['bleu']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kocScltnxfn9",
        "outputId": "d6fb8394-1fee-4c55-c60a-f0b5eefebec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ STEP 7: Calculating lexical similarity metrics...\n",
            "\n",
            "--- ROUGE Scores ---\n",
            "  rouge1  : 0.3171\n",
            "  rouge2  : 0.2201\n",
            "  rougeL  : 0.2676\n",
            "  rougeLsum: 0.2679\n",
            "\n",
            "--- BLEU Score ---\n",
            "  BLEU    : 0.0935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"✅ STEP 8: Calculating semantic similarity...\")\n",
        "\n",
        "# Load a pre-trained model for creating sentence embeddings\n",
        "similarity_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
        "\n",
        "# Generate embeddings for both lists of insights\n",
        "generated_embeddings = similarity_model.encode(generated_insights, convert_to_tensor=True)\n",
        "reference_embeddings = similarity_model.encode(reference_insights, convert_to_tensor=True)\n",
        "\n",
        "# Compute cosine similarity between each pair of embeddings\n",
        "cosine_scores = util.cos_sim(generated_embeddings, reference_embeddings)\n",
        "\n",
        "# Extract the diagonal scores (the similarity of each generated text with its corresponding reference)\n",
        "semantic_scores = [cosine_scores[i][i].item() for i in range(len(generated_insights))]\n",
        "average_semantic_score = np.mean(semantic_scores)\n",
        "\n",
        "print(\"\\n--- Semantic Similarity Score ---\")\n",
        "print(f\"  Average Cosine Similarity: {average_semantic_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vdd9mbSgx3yJ",
        "outputId": "1514e50e-452b-40f7-b074-5af6b4c668ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ STEP 8: Calculating semantic similarity...\n",
            "\n",
            "--- Semantic Similarity Score ---\n",
            "  Average Cosine Similarity: 0.7159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# CELL 9: DISPLAY AND ANALYZE EXAMPLE RESULTS (CORRECTED)\n",
        "#\n",
        "print(\"✅ STEP 9: Qualitative analysis of generated examples...\")\n",
        "\n",
        "# Create a DataFrame for easy comparison\n",
        "results_df = pd.DataFrame({\n",
        "    # THIS IS THE CORRECTED LINE:\n",
        "    'Input Log Data': [create_prompt(test_df.iloc[i]).split('**Actual Input:**')[1].split('**Actual Insight:**')[0].strip() for i in range(5)],\n",
        "    'Reference Insight (Ground Truth)': reference_insights[:5],\n",
        "    'Generated Insight (Flan-T5)': generated_insights[:5],\n",
        "    'Semantic Score': semantic_scores[:5]\n",
        "})\n",
        "\n",
        "# Set pandas display options for better readability of long text\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.width', 200)\n",
        "\n",
        "print(\"\\n--- Comparison of Top 5 Results ---\")\n",
        "display(results_df.style.set_properties(**{'text-align': 'left', 'white-space': 'normal'}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "id": "WJzS1_Rrx7SA",
        "outputId": "9fef8754-67b7-4821-e214-719f2211f0c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ STEP 9: Qualitative analysis of generated examples...\n",
            "\n",
            "--- Comparison of Top 5 Results ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7af674864e10>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_ac143_row0_col0, #T_ac143_row0_col1, #T_ac143_row0_col2, #T_ac143_row0_col3, #T_ac143_row1_col0, #T_ac143_row1_col1, #T_ac143_row1_col2, #T_ac143_row1_col3, #T_ac143_row2_col0, #T_ac143_row2_col1, #T_ac143_row2_col2, #T_ac143_row2_col3, #T_ac143_row3_col0, #T_ac143_row3_col1, #T_ac143_row3_col2, #T_ac143_row3_col3, #T_ac143_row4_col0, #T_ac143_row4_col1, #T_ac143_row4_col2, #T_ac143_row4_col3 {\n",
              "  text-align: left;\n",
              "  white-space: normal;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_ac143\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_ac143_level0_col0\" class=\"col_heading level0 col0\" >Input Log Data</th>\n",
              "      <th id=\"T_ac143_level0_col1\" class=\"col_heading level0 col1\" >Reference Insight (Ground Truth)</th>\n",
              "      <th id=\"T_ac143_level0_col2\" class=\"col_heading level0 col2\" >Generated Insight (Flan-T5)</th>\n",
              "      <th id=\"T_ac143_level0_col3\" class=\"col_heading level0 col3\" >Semantic Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_ac143_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_ac143_row0_col0\" class=\"data row0 col0\" >Machine Id: PROD-SRV-1001\n",
              "Cpu Usage Percent: 95.1\n",
              "Memory Usage Gb: 16.94\n",
              "Disk Io Mbps: 124.28\n",
              "Error Count: 0</td>\n",
              "      <td id=\"T_ac143_row0_col1\" class=\"data row0 col1\" >High CPU usage (95.1%) on PROD-SRV-1001 suggests a potential performance bottleneck. Recommend analyzing top processes.</td>\n",
              "      <td id=\"T_ac143_row0_col2\" class=\"data row0 col2\" >The CPU usage (95.1%) on PROD-SRV-1001 suggests a potential performance bottleneck. Recommend analyzing top processes. ---</td>\n",
              "      <td id=\"T_ac143_row0_col3\" class=\"data row0 col3\" >0.980874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ac143_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_ac143_row1_col0\" class=\"data row1 col0\" >Machine Id: PROD-SRV-1002\n",
              "Cpu Usage Percent: 32.25\n",
              "Memory Usage Gb: 13.95\n",
              "Disk Io Mbps: 237.53\n",
              "Error Count: 0</td>\n",
              "      <td id=\"T_ac143_row1_col1\" class=\"data row1 col1\" >Unusually high disk I/O (237.53 MB/s) on PROD-SRV-1002 could degrade storage performance. Monitor read/write operations.</td>\n",
              "      <td id=\"T_ac143_row1_col2\" class=\"data row1 col2\" >The CPU usage on PROD-SRV-1002 was 32.25 percent. The memory usage was 13.95 percent and the disk Io Mbps was 237.53. The error count was 0.</td>\n",
              "      <td id=\"T_ac143_row1_col3\" class=\"data row1 col3\" >0.718645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ac143_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_ac143_row2_col0\" class=\"data row2 col0\" >Machine Id: PROD-SRV-1003\n",
              "Cpu Usage Percent: 97.75\n",
              "Memory Usage Gb: 30.24\n",
              "Disk Io Mbps: 236.14\n",
              "Error Count: 10</td>\n",
              "      <td id=\"T_ac143_row2_col1\" class=\"data row2 col1\" >Critical alert: 10 errors detected on PROD-SRV-1003. Immediate investigation is required to ensure system stability.</td>\n",
              "      <td id=\"T_ac143_row2_col2\" class=\"data row2 col2\" >Machine Id: PROD-SRV-1003 has a CPU usage of 97.75 percent, Memory Usage Gb: 30.24 Disk Io Mbps: 236.14 and an error count of 10.</td>\n",
              "      <td id=\"T_ac143_row2_col3\" class=\"data row2 col3\" >0.602368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ac143_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_ac143_row3_col0\" class=\"data row3 col0\" >Machine Id: PROD-SRV-1004\n",
              "Cpu Usage Percent: 92.05\n",
              "Memory Usage Gb: 19.46\n",
              "Disk Io Mbps: 62.51\n",
              "Error Count: 1</td>\n",
              "      <td id=\"T_ac143_row3_col1\" class=\"data row3 col1\" >High CPU usage (92.05%) on PROD-SRV-1004 suggests a potential performance bottleneck. Recommend analyzing top processes.</td>\n",
              "      <td id=\"T_ac143_row3_col2\" class=\"data row3 col2\" >The CPU usage percentage of PROD-SRV-1004 is 92.05. The memory usage is 19.46 GB and the disk Io Mbps are 62.51.</td>\n",
              "      <td id=\"T_ac143_row3_col3\" class=\"data row3 col3\" >0.794480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ac143_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_ac143_row4_col0\" class=\"data row4 col0\" >Machine Id: PROD-SRV-1005\n",
              "Cpu Usage Percent: 51.78\n",
              "Memory Usage Gb: 17.61\n",
              "Disk Io Mbps: 91.28\n",
              "Error Count: 0</td>\n",
              "      <td id=\"T_ac143_row4_col1\" class=\"data row4 col1\" >System PROD-SRV-1005 is operating within normal performance parameters. No action is required at this time.</td>\n",
              "      <td id=\"T_ac143_row4_col2\" class=\"data row4 col2\" >The CPU usage percentage of PROD-SRV-1005 is 51.78. The memory usage is 17.61 GB and the disk Io Mbps is 91.28.</td>\n",
              "      <td id=\"T_ac143_row4_col3\" class=\"data row4 col3\" >0.627327</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"✅ Final Step: Generating the .txt report file...\")\n",
        "\n",
        "def generate_report_text(model_name, rouge_scores, bleu_score, sem_score, results_dataframe, prompt_func, sample_row):\n",
        "    \"\"\"\n",
        "    Formats all experiment results into a single string for the report.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- SECTION 1: MODEL DETAILS ---\n",
        "    section1 = f\"\"\"\n",
        "======================================================================\n",
        "         ASSIGNMENT 4: TABLE-TO-INSIGHTS GENERATION REPORT\n",
        "======================================================================\n",
        "\n",
        "----------------------------------------------------------------------\n",
        "SECTION 1: MODEL DETAILS\n",
        "----------------------------------------------------------------------\n",
        "\n",
        "1.  **Model Name**: {model_name}\n",
        "\n",
        "2.  **Model Architecture**: The model is based on the Text-to-Text Transfer Transformer (T5) architecture.\n",
        "    It is an encoder-decoder model that is pre-trained on a mixture of unsupervised and supervised tasks\n",
        "    and then fine-tuned on a large collection of instruction-based datasets (\"instruction tuning\").\n",
        "\n",
        "3.  **Number of Parameters**: The 'large' version has approximately 780 Million parameters.\n",
        "\n",
        "4.  **Reason for Choice**:\n",
        "    * **Instruction-Tuned**: Flan-T5 is designed to perform well on unseen tasks by following natural\n",
        "        language instructions (prompts), which is ideal for this assignment.\n",
        "    * **Performance**: The 'large' variant offers a strong balance between high performance and the\n",
        "        ability to run on freely available hardware like Google Colab's T4 GPU.\n",
        "\"\"\"\n",
        "\n",
        "    # --- SECTION 2: EXPERIMENT RESULTS ---\n",
        "\n",
        "    # Get the prompt template structure\n",
        "    prompt_template = prompt_func(sample_row).split(\"--- LOG DATA ---\")[0] + \"\\\\n--- LOG DATA ---\\\\n{LOG DATA}\\\\n--- INSIGHT ---\"\n",
        "\n",
        "    # Format the metrics\n",
        "    metrics_text = f\"\"\"\n",
        "----------------------------------------------------------------------\n",
        "SECTION 2: EXPERIMENT RESULTS\n",
        "----------------------------------------------------------------------\n",
        "\n",
        "1.  **Prompt Used for Generation**:\n",
        "    The following prompt template was used to instruct the model. Each row from the\n",
        "    test data was formatted and inserted into the {{LOG DATA}} placeholder.\n",
        "\n",
        "    ```\n",
        "    {prompt_template}\n",
        "    ```\n",
        "\n",
        "2.  **Performance Metrics**:\n",
        "    * **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**:\n",
        "        - ROUGE-1 (Unigram Overlap):   {rouge_scores['rouge1']:.4f}\n",
        "        - ROUGE-2 (Bigram Overlap):    {rouge_scores['rouge2']:.4f}\n",
        "        - ROUGE-L (Longest Subsequence): {rouge_scores['rougeL']:.4f}\n",
        "\n",
        "    * **BLEU (Bilingual Evaluation Understudy)**:\n",
        "        - BLEU Score:                  {bleu_score['bleu']:.4f}\n",
        "\n",
        "    * **Semantic Match Score**:\n",
        "        - Average Cosine Similarity:   {sem_score:.4f}\n",
        "\"\"\"\n",
        "\n",
        "    # Format the five examples\n",
        "    examples_text = \"\\n3.  **Five Example Results**:\\n\"\n",
        "    for index, row in results_dataframe.head(5).iterrows():\n",
        "        examples_text += f\"\"\"\n",
        "    ------------------------------- Example {index + 1} -------------------------------\n",
        "\n",
        "    **INPUT LOG DATA**:\n",
        "    {row['Input Log Data']}\n",
        "\n",
        "    **REFERENCE INSIGHT (Ground Truth)**:\n",
        "    {row['Reference Insight (Ground Truth)']}\n",
        "\n",
        "    **GENERATED INSIGHT (Flan-T5)**:\n",
        "    {row['Generated Insight (Flan-T5)']}\n",
        "\n",
        "    **SEMANTIC SIMILARITY SCORE**: {row['Semantic Score']:.4f}\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Combine all sections ---\n",
        "    final_report = section1 + metrics_text + examples_text + \"\\n\\n========================= END OF REPORT =========================\"\n",
        "    return final_report.strip()\n",
        "\n",
        "\n",
        "# --- Main execution ---\n",
        "# Gather all the necessary variables from the notebook\n",
        "report_content = generate_report_text(\n",
        "    model_name=MODEL_NAME,\n",
        "    rouge_scores=rouge_results,\n",
        "    bleu_score=bleu_results,\n",
        "    sem_score=average_semantic_score,\n",
        "    results_dataframe=results_df,\n",
        "    prompt_func=create_prompt,\n",
        "    sample_row=test_df.iloc[0] # Used to generate the prompt template\n",
        ")\n",
        "\n",
        "# Write the report string to a .txt file\n",
        "report_filename = \"assignment_report.txt\"\n",
        "with open(report_filename, 'w', encoding='utf-8') as f:\n",
        "    f.write(report_content)\n",
        "\n",
        "print(f\"\\n✅ Report successfully generated and saved as '{report_filename}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIgK57AuyADs",
        "outputId": "4f616b18-147f-401f-f7fe-df0899afc61d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Final Step: Generating the .txt report file...\n",
            "\n",
            "✅ Report successfully generated and saved as 'assignment_report.txt'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nvw_vCbMyU0k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}